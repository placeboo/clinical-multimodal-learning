{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Cm8-s2VcPpSd"},"outputs":[],"source":["import pandas as pd\n","import os\n","import numpy as np\n","from gensim.models import Word2Vec, FastText\n","import glove\n","from glove import Corpus\n","\n","import collections\n","import gc \n","\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YBUc5CNjPpSd"},"outputs":[],"source":["new_notes = pd.read_pickle(\"data/ner_df.p\") # med7\n","w2vec = Word2Vec.load(\"embeddings/word2vec.model\")\n","fasttext = FastText.load(\"embeddings/fasttext.model\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IZqwyivoPpSd"},"outputs":[],"source":["null_index_list = []\n","for i in new_notes.itertuples():\n","    \n","    if len(i.ner) == 0:\n","        null_index_list.append(i.Index)\n","new_notes.drop(null_index_list, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HxM-YKyVPpSe"},"outputs":[],"source":["med7_ner_data = {}\n","\n","for ii in new_notes.itertuples():\n","    \n","    p_id = ii.SUBJECT_ID\n","    ind = ii.Index\n","    \n","    try:\n","        new_ner = new_notes.loc[ind].ner\n","    except:\n","        new_ner = []\n","            \n","    unique = set()\n","    new_temp = []\n","    \n","    for j in new_ner:\n","        for k in j:\n","            \n","            unique.add(k[0])\n","            new_temp.append(k)\n","\n","    if p_id in med7_ner_data:\n","        for i in new_temp:\n","            med7_ner_data[p_id].append(i)\n","    else:\n","        med7_ner_data[p_id] = new_temp"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QwTjXatBPpSe"},"outputs":[],"source":["pd.to_pickle(med7_ner_data, \"data/new_ner_word_dict.pkl\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yj_7hBQYPpSe"},"outputs":[],"source":["def mean(a):\n","    return sum(a) / len(a)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aw6rq_JmPpSe"},"outputs":[],"source":["data_types = [med7_ner_data]\n","data_names = [\"new_ner\"]\n","\n","for data, names in zip(data_types, data_names):\n","    new_word2vec = {}\n","    print(\"w2vec starting..\")\n","    for k,v in data.items():\n","\n","        patient_temp = []\n","        for i in v:\n","            try:\n","                patient_temp.append(w2vec[i[0]])\n","            except:\n","                avg = []\n","                num = 0\n","                temp = []\n","\n","                if len(i[0].split(\" \")) > 1:\n","                    for each_word in i[0].split(\" \"):\n","                        try:\n","                            temp = w2vec[each_word]\n","                            avg.append(temp)\n","                            num += 1\n","                        except:\n","                            pass\n","                    if num == 0: continue\n","                    avg = np.asarray(avg)\n","                    t = np.asarray(map(mean, zip(*avg)))\n","                    patient_temp.append(t)\n","        if len(patient_temp) == 0: continue\n","        new_word2vec[k] = patient_temp\n","\n","    #############################################################################\n","    # print(\"fasttext starting..\")\n","        \n","    # new_fasttextvec = {}\n","\n","    # for k,v in data.items():\n","\n","    #     patient_temp = []\n","\n","    #     for i in v:\n","    #         try:\n","    #             patient_temp.append(fasttext[i[0]])\n","    #         except:\n","    #             pass\n","    #     if len(patient_temp) == 0: continue\n","    #     new_fasttextvec[k] = patient_temp\n","\n","    # #############################################################################    \n","        \n","    # print(\"combined starting..\")\n","    # new_concatvec = {}\n","\n","    # for k,v in data.items():\n","    #     patient_temp = []\n","    # #     if k != 6: continue\n","    #     for i in v:\n","    #         w2vec_temp = []\n","    #         try:\n","    #             w2vec_temp = w2vec[i[0]]\n","    #         except:\n","    #             avg = []\n","    #             num = 0\n","    #             temp = []\n","\n","    #             if len(i[0].split(\" \")) > 1:\n","    #                 for each_word in i[0].split(\" \"):\n","    #                     try:\n","    #                         temp = w2vec[each_word]\n","    #                         avg.append(temp)\n","    #                         num += 1\n","    #                     except:\n","    #                         pass\n","    #                 if num == 0: \n","    #                     w2vec_temp = [0] * 100\n","    #                 else:\n","    #                     avg = np.asarray(avg)\n","    #                     w2vec_temp = np.asarray(map(mean, zip(*avg)))\n","    #             else:\n","    #                 w2vec_temp = [0] * 100\n","\n","    #         fasttemp = fasttext[i[0]]\n","\n","    #         appended = np.append(fasttemp, w2vec_temp, 0)\n","    #         patient_temp.append(appended)\n","    #     if len(patient_temp) == 0: continue\n","    #     new_concatvec[k] = patient_temp\n","\n","    # print(len(new_word2vec), len(new_fasttextvec), len(new_concatvec))\n","    print(len(new_word2vec))\n","    pd.to_pickle(new_word2vec, \"data/\"+names+\"_word2vec_dict.pkl\")\n","    # pd.to_pickle(new_fasttextvec, \"data/\"+names+\"_fasttext_dict.pkl\")\n","    # pd.to_pickle(new_concatvec, \"data/\"+names+\"_combined_dict.pkl\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-GNXCV0RPpSe"},"outputs":[],"source":["# diff = set(new_fasttext_dict.keys()).difference(set(new_word2vec_dict))\n","# for i in diff:\n","#     del new_fasttext_dict[i]\n","#     del new_combined_dict[i]\n","# print (len(new_word2vec_dict), len(new_fasttext_dict), len(new_combined_dict))\n","\n","\n","pd.to_pickle(new_word2vec_dict, \"data/\"+\"new_ner\"+\"_word2vec_limited_dict.pkl\")\n","# pd.to_pickle(new_fasttext_dict, \"data/\"+\"new_ner\"+\"_fasttext_limited_dict.pkl\")\n","# pd.to_pickle(new_combined_dict, \"data/\"+\"new_ner\"+\"_combined_limited_dict.pkl\")"]}],"metadata":{"kernelspec":{"display_name":"Python 2","language":"python","name":"python2"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.12"},"colab":{"provenance":[{"file_id":"https://github.com/tanlab/ConvolutionMedicalNer/blob/master/05-Represent-Entities-With-Different-Embeddings.ipynb","timestamp":1680410088009}]}},"nbformat":4,"nbformat_minor":0}